{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Example 1\n",
    "=========\n",
    "\n",
    "Let us test ``cgn`` on a simple unconstrained linear least-squares problem. As an example, we use a regularized version of problem 32 from the article\n",
    "\n",
    "More, J., Garbox, B. and Hillstrom, E. \"Testing Unconstrained Optimization Software\", 1981.\n",
    "\n",
    "The problem is as follows:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\min_{x \\in \\mathbb R^n} & \\frac{1}{2}||F(x)||_2^2 + \\frac{\\beta}{2}||x||_2^2, \\\\\n",
    "\\text{where} \\quad & F(x)_j = x_j - \\frac{2}{m} \\sum_{i=1}^n x_i - 1, \\quad \\text{for } 1 \\leq j \\leq n, \\\\\n",
    "& F(x)_j = - \\frac{2}{m} \\sum_{i=1}^n x_i - 1, \\quad \\text{for } n < j <= m, \\\\\n",
    "& \\beta = \\frac{1}{100},\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "with $m >= n$. Let us choose $n=200$ and $m=400$."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we implement the affine misfit function $F=F(x)$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = 400\n",
    "n = 200\n",
    "\n",
    "def F(x):\n",
    "    z = np.zeros(m)\n",
    "    for i in range(n):\n",
    "        z[i] = x[i] - 2.0 * sum(x) / m - 1.\n",
    "    for i in range(n,m):\n",
    "        z[i] = - 2.0 * sum(x) / m - 1.\n",
    "    return z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also have to implement its Jacobian:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "A = np.zeros((m,n))\n",
    "# upper half of the Jacobian matrix\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            A[i,j] = 1.0 - 2.0 / m\n",
    "        else:\n",
    "            A[i,j] = -2.0 / m\n",
    "# lower half of the Jacobian matrix\n",
    "for i in  range(n,m):\n",
    "    for j in range(n):\n",
    "        A[i,j] = -2.0 / m\n",
    "\n",
    "def DF(x):\n",
    "    return A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now set up the ``cgn.Parameter`` object. To create a ``cgn.Parameter`` object, the user has to provide a starting guess at initialization. This is necessary so that ``cgn`` can verify the consistency of the user input (e.g. that the dimensions match) before it starts solving the optimization problem. By doing that, it makes it easier to diagnose runtime errors that occur later. For our parameter $x$, we simply use the 1-vector as initial guess."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import cgn\n",
    "\n",
    "x = cgn.Parameter(start=np.ones(n), name=\"x\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the starting guess can be adapted later via the ``x.start``-property.\n",
    "Next, we set up the regularization term by setting $\\beta = 1$. The regularization operator defaults to the identity, and the regularizing guess default to the zero vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "x.beta = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we can initialize the ``cgn.Problem`` object:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "problem = cgn.Problem(parameters=[x], fun=F, jac=DF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We initialize the solver..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "solver = cgn.CGN()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and solve the problem:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting the constrained Gauss-Newton method. Cost at starting value: 500.1\n",
      "+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n",
      "| Iteration | Cost                    | Constraint violation    | Stepsize (||p||)        | Steplength (h)          | Computation time [s]    |\n",
      "+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n",
      "+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n",
      "|     1     |    100.09990009990057   |           0.0           |    28.270143239845737   |           1.0           |   0.012190103530883789  |\n",
      "+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n",
      "|     2     |    100.0999000999005    |           0.0           |   1.55683666436841e-13  |           0.25          |   0.03589987754821777   |\n",
      "+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+\n",
      "The iteration converged successfully after 2 steps.\n",
      "Minimum of cost function: 100.0999000999005\n"
     ]
    }
   ],
   "source": [
    "x_start = np.ones(n)\n",
    "solver.options.set_verbosity(2)\n",
    "solution = solver.solve(problem=problem)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us view the solution:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001, -0.999001, -0.999001, -0.999001, -0.999001,\n       -0.999001, -0.999001])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min = solution.minimizer(\"x\")\n",
    "x_min"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us compare this solution to the one obtained with the implementation of ridge regression from ``scikit-learn``:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference to ridge: 3.467185801042848e-14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "clf = Ridge(alpha=x.beta, fit_intercept=False)\n",
    "y = np.ones(m)\n",
    "clf.fit(X=A, y=y)\n",
    "x_ridge = clf.coef_\n",
    "\n",
    "difference_to_ridge = np.linalg.norm(x_min - x_ridge)\n",
    "print(f\"Difference to ridge: {difference_to_ridge}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two solutions agree up to a precision of $10^{-13}$!\n",
    "\n",
    "Note that the ``solution`` object also provides us access to the minimum of the cost function. Here, it is important to keep in mind that the cost function always has a factor $\\frac{1}{2}$ in front."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at minimum: 100.0999000999005\n"
     ]
    }
   ],
   "source": [
    "cost_at_minimum = solution.cost\n",
    "print(f\"Cost at minimum: {cost_at_minimum}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us verify this by manually recomputing the cost at ``x_min``:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at minimum, recomputed: 100.0999000999005\n"
     ]
    }
   ],
   "source": [
    "cost_recomputed = 0.5 * np.sum(np.square(F(x_min))) + 0.5 * x.beta * np.sum(np.square(x_min))\n",
    "print(f\"Cost at minimum, recomputed: {cost_at_minimum}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And indeed, both numbers agree.\n",
    "\n",
    "Finally, the ``solution`` also provides access to the [precision matrix](https://en.wikipedia.org/wiki/Precision_(statistics)) via ``solution.precision``.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "(200, 200)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = solution.precision\n",
    "precision.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is relevant in the case where our optimization problem comes from [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) or\n",
    "[Bayesian maximum-a-posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), and the cost function actually corresponds to a negative log-likelihood or a negative log-posterior density, respectively."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}